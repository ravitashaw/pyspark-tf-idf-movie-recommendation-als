{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common word occurece in text using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Source\n",
    "\n",
    "* Portrait: https://www.gutenberg.org/files/4217/4217-0.txt\n",
    "* Ulysses: https://www.gutenberg.org/files/4300/4300-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2060</td><td>application_1572292571909_0542</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0542/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-782.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0542_01_000001/user12049\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a little info about our session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '2048M', 'executorCores': 2, 'proxyUser': 'user12049', 'conf': {'spark.master': 'yarn-client'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2047</td><td>application_1572292571909_0529</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0529/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-792.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0529_01_000001/user12067\">Link</a></td><td></td></tr><tr><td>2050</td><td>application_1572292571909_0532</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0532/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-782.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0532_01_000001/user12069\">Link</a></td><td></td></tr><tr><td>2051</td><td>application_1572292571909_0533</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0533/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-782.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0533_01_000001/user12046\">Link</a></td><td></td></tr><tr><td>2053</td><td>application_1572292571909_0535</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0535/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-786.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0535_01_000002/user12026\">Link</a></td><td></td></tr><tr><td>2054</td><td>application_1572292571909_0536</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0536/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-782.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0536_01_000001/user12056\">Link</a></td><td></td></tr><tr><td>2055</td><td>application_1572292571909_0537</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0537/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-781.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0537_01_000001/user12031\">Link</a></td><td></td></tr><tr><td>2056</td><td>application_1572292571909_0538</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0538/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-790.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0538_01_000001/user12056\">Link</a></td><td></td></tr><tr><td>2057</td><td>application_1572292571909_0539</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0539/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-787.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0539_01_000001/user12009\">Link</a></td><td></td></tr><tr><td>2059</td><td>application_1572292571909_0541</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0541/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-786.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0541_01_000001/user12061\">Link</a></td><td></td></tr><tr><td>2060</td><td>application_1572292571909_0542</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1572292571909_0542/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-782.oit.duke.edu:8042/node/containerlogs/container_e26_1572292571909_0542_01_000001/user12049\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn a little more about sparkmagic commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td>%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td>Executes spark commands.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the variable sqlContext (Spark v1.x) or spark (Spark v2.x).\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the SQL query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m, -n, -r are the same as the %%spark parameters above.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StopWords - to remove from from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 hdfs hdfs     496309 2018-10-11 11:04 /data/texts/Portrait.txt\n",
      "-rw-r--r--   3 hdfs hdfs    1580890 2018-10-11 11:04 /data/texts/Ulysses.txt"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = 'hdfs dfs -ls /data/texts'.split() \n",
    "files = subprocess.check_output(cmd).strip().split('\\n')\n",
    "for path in files:\n",
    "    print (path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "|file                                                     |\n",
      "+---------------------------------------------------------+\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "|hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt|\n",
      "+---------------------------------------------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "raw_lines = spark.read.text('/data/texts')\\\n",
    "    .withColumn('file', f.input_file_name())\n",
    "raw_lines.select('file').show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|               value|   book|\n",
      "+--------------------+-------+\n",
      "|                    |Ulysses|\n",
      "|The Project Guten...|Ulysses|\n",
      "|                    |Ulysses|\n",
      "|This eBook is for...|Ulysses|\n",
      "|no restrictions w...|Ulysses|\n",
      "|it under the term...|Ulysses|\n",
      "|eBook or online a...|Ulysses|\n",
      "|                    |Ulysses|\n",
      "|                    |Ulysses|\n",
      "|      Title: Ulysses|Ulysses|\n",
      "|                    |Ulysses|\n",
      "| Author: James Joyce|Ulysses|\n",
      "|                    |Ulysses|\n",
      "|Release Date: Aug...|Ulysses|\n",
      "|Last Updated: Aug...|Ulysses|\n",
      "|                    |Ulysses|\n",
      "|   Language: English|Ulysses|\n",
      "|                    |Ulysses|\n",
      "|Character set enc...|Ulysses|\n",
      "|                    |Ulysses|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "lines_by_book = raw_lines \\\n",
    "    .withColumn('book', f.regexp_extract('file', '^.*/(.*)\\.txt$', 1))\\\n",
    "    .drop('file')\n",
    "lines_by_book.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    book| count|\n",
      "+--------+------+\n",
      "| Ulysses|278497|\n",
      "|Portrait| 90866|\n",
      "+--------+------+"
     ]
    }
   ],
   "source": [
    "# Counting the total number of words in each book\n",
    "lines_by_book.withColumn('word', f.explode(f.split(f.col('value'), ' ')))\\\n",
    "    .groupBy('book')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_book = lines_by_book.withColumn('word', f.explode(f.split(f.lower(f.col('value')), '[^a-z]'))).drop('value')\\\n",
    "                            .where(f.length('word')>0)\\\n",
    "                            .filter(~f.col('word').isin(StopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   book|        word|\n",
      "+-------+------------+\n",
      "|Ulysses|     project|\n",
      "|Ulysses|   gutenberg|\n",
      "|Ulysses|       ebook|\n",
      "|Ulysses|     ulysses|\n",
      "|Ulysses|       james|\n",
      "|Ulysses|       joyce|\n",
      "|Ulysses|       ebook|\n",
      "|Ulysses|         use|\n",
      "|Ulysses|      anyone|\n",
      "|Ulysses|    anywhere|\n",
      "|Ulysses|        cost|\n",
      "|Ulysses|      almost|\n",
      "|Ulysses|restrictions|\n",
      "|Ulysses|  whatsoever|\n",
      "|Ulysses|         may|\n",
      "|Ulysses|        copy|\n",
      "|Ulysses|        give|\n",
      "|Ulysses|        away|\n",
      "|Ulysses|         use|\n",
      "|Ulysses|       terms|\n",
      "+-------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Showing top words in each text\n",
    "words_by_book.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|   said| 1823|\n",
      "|stephen| 1004|\n",
      "|  bloom| 1001|\n",
      "|    one|  965|\n",
      "|   like|  951|\n",
      "|     mr|  855|\n",
      "|    old|  603|\n",
      "|    man|  563|\n",
      "|    see|  521|\n",
      "|   eyes|  502|\n",
      "|   time|  498|\n",
      "|   says|  494|\n",
      "| father|  480|\n",
      "|   back|  479|\n",
      "|    god|  470|\n",
      "|    two|  470|\n",
      "|    yes|  434|\n",
      "|   know|  432|\n",
      "| little|  420|\n",
      "|   good|  412|\n",
      "+-------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Word count in both the texts\n",
    "words_by_book.groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Term Frequency (TF) for each book-word combination.  \n",
    "  \n",
    "Term Frequency (TF) = word count measure normalized by document. If we let  \n",
    "    $n_w$ = the  number of occurrences of a particular word in a document and  \n",
    "    $n_d$ = the total number of words in a document  \n",
    "  \n",
    "  \n",
    "$$TF = n_w / n_d$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+------+--------------------+\n",
      "|    book|   word| n_w|   n_d|                  tf|\n",
      "+--------+-------+----+------+--------------------+\n",
      "| Ulysses|   said|1208|145565|0.008298698176072546|\n",
      "| Ulysses|  bloom|1000|145565|  0.0068697832583382|\n",
      "| Ulysses|    one| 742|145565|0.005097379177686944|\n",
      "| Ulysses|   like| 731|145565|0.005021811561845224|\n",
      "| Ulysses|     mr| 719|145565|0.004939374162745165|\n",
      "|Portrait|   said| 615| 43044|0.014287705603568441|\n",
      "| Ulysses|stephen| 571|145565|0.003922646240511112|\n",
      "| Ulysses|    old| 492|145565|0.003379933363102394|\n",
      "| Ulysses|   says| 473|145565|0.003249407481193...|\n",
      "| Ulysses|    man| 451|145565|0.003098272249510528|\n",
      "| Ulysses|    see| 435|145565|0.002988355717377...|\n",
      "|Portrait|stephen| 433| 43044|0.010059474026577456|\n",
      "| Ulysses|    two| 391|145565|0.002686085254010236|\n",
      "| Ulysses|   time| 380|145565|0.002610517638168516|\n",
      "| Ulysses|   back| 362|145565|0.002486861539518428|\n",
      "| Ulysses|    yes| 359|145565|0.002466252189743...|\n",
      "| Ulysses|   eyes| 329|145565|0.002260158691993...|\n",
      "| Ulysses|   know| 328|145565|0.002253288908734...|\n",
      "| Ulysses|   good| 321|145565|0.002205200425926562|\n",
      "| Ulysses|   hand| 308|145565|0.002115893243568...|\n",
      "+--------+-------+----+------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy(words_by_book['book'])\n",
    "\n",
    "book_tf = words_by_book.groupBy('book', 'word')\\\n",
    "    .agg(f.count('*').alias('n_w'),\n",
    "         f.sum(f.count('*')).over(w).alias('n_d'),\n",
    "         (f.count('*')/f.sum(f.count('*')).over(w)).alias('tf')\n",
    "        )\\\n",
    "    .orderBy('n_w', ascending=False)\\\n",
    "    .cache()\n",
    "\n",
    "book_tf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Inverse Documents Frequency (IDF) for each book-word combination.\n",
    "    $c_d$ = the total number of documents   \n",
    "    $i_d$ = the number of documents with a given term in them  \n",
    "\n",
    "then we define IDF as  \n",
    "  \n",
    "$$IDF = log(c_d / i_d)$$\n",
    "     \n",
    "wheren $log$ is a natural logarithm with base e.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w_idf = Window.partitionBy(words_by_book['book'])\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w_d = Window.partitionBy(words_by_book['word'])\n",
    "\n",
    "c_d = words_by_book.select('book').distinct().count()\n",
    "\n",
    "book_idf = words_by_book.groupBy('book', 'word')\\\n",
    "    .agg(f.lit(c_d).alias('c_d'),\n",
    "         f.count('*').over(w_d).alias('i_d'),\n",
    "         (f.log(f.lit(c_d)/f.count('*').over(w_d))).alias('idf')\n",
    "        )\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---+---+------------------+\n",
      "|    book|         word|c_d|i_d|               idf|\n",
      "+--------+-------------+---+---+------------------+\n",
      "| Ulysses| accumulation|  2|  1|0.6931471805599453|\n",
      "|Portrait|apprehensions|  2|  1|0.6931471805599453|\n",
      "| Ulysses|       arator|  2|  1|0.6931471805599453|\n",
      "| Ulysses|      arclamp|  2|  1|0.6931471805599453|\n",
      "| Ulysses|          art|  2|  2|               0.0|\n",
      "|Portrait|          art|  2|  2|               0.0|\n",
      "| Ulysses|        baaaa|  2|  1|0.6931471805599453|\n",
      "| Ulysses|     baddybad|  2|  1|0.6931471805599453|\n",
      "| Ulysses|       bazaar|  2|  1|0.6931471805599453|\n",
      "| Ulysses|       biting|  2|  1|0.6931471805599453|\n",
      "|Portrait|     blackish|  2|  1|0.6931471805599453|\n",
      "| Ulysses|  bloohimwhom|  2|  1|0.6931471805599453|\n",
      "| Ulysses|      blossom|  2|  1|0.6931471805599453|\n",
      "| Ulysses|       brands|  2|  1|0.6931471805599453|\n",
      "| Ulysses|     cactuses|  2|  1|0.6931471805599453|\n",
      "| Ulysses|        carlo|  2|  1|0.6931471805599453|\n",
      "| Ulysses|     cautious|  2|  1|0.6931471805599453|\n",
      "| Ulysses|    cesspools|  2|  1|0.6931471805599453|\n",
      "| Ulysses|       ceylon|  2|  1|0.6931471805599453|\n",
      "|Portrait|   citadelled|  2|  1|0.6931471805599453|\n",
      "+--------+-------------+---+---+------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "book_idf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
